{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-22T07:30:52.422953Z",
     "start_time": "2025-01-22T07:30:40.182253Z"
    }
   },
   "source": [
    "import os\n",
    "import transformers\n",
    "import datasets\n",
    "import torch\n",
    "import inspect\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, Dataset\n",
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import *\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import TrainingArguments as TrainingArgumentsBase\n",
    "from transformers import IntervalStrategy, SchedulerType\n",
    "from transformers.training_args import OptimizerNames\n",
    "from transformers.utils.generic import PaddingStrategy\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from transformers.debug_utils import DebugOption\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    PreTrainedTokenizerBase,\n",
    "    PreTrainedModel,\n",
    "    AutoModel,\n",
    "    DataCollatorWithPadding,\n",
    "    default_data_collator,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T04:11:14.168463Z",
     "start_time": "2025-01-11T04:11:14.137637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# parser args\n",
    "@dataclass\n",
    "class TrainingArguments(TrainingArgumentsBase):\n",
    "    # general\n",
    "    output_dir: str = field(default='output/0')\n",
    "    overwrite_output_dir: bool = field(default=True)\n",
    "    do_train: bool = field(default=True)\n",
    "    do_eval: bool = field(default=True)\n",
    "    do_predict: bool = field(default=True)\n",
    "    seed: int = field(\n",
    "        default=42,\n",
    "        metadata={\n",
    "            \"help\": \"Random seed that will be set at the beginning of training.\"\n",
    "        }\n",
    "    )\n",
    "    load_best_model_at_end: bool = field(default=True)\n",
    "    fp16: bool = field(default=True)  # precision\n",
    "    metric_for_best_model: str = field(default='eval_spearmanr')\n",
    "    include_inputs_for_metrics: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": \"Whether or not the inputs will be passed to the `compute_metrics` function.\"\n",
    "        }\n",
    "    )  # for extra info\n",
    "    # greater_is_better: Optional[bool] = field(default=None,\n",
    "    #     metadata={\n",
    "    #         \"help\": (\n",
    "    #             \"Whether the `metric_for_best_model` should be maximized or not.\"\n",
    "    #             \"- `True` if doesn't end in `'loss'`.\"\n",
    "    #             \"- `False` if ends in `'loss'`.\"\n",
    "    #         )\n",
    "    #     })\n",
    "    # debug: Union[str, List[DebugOption]] = field(default=\"underflow_overflow\",\n",
    "    #     metadata={\n",
    "    #         \"help\": (\n",
    "    #             \"Whether or not to enable debug mode. Current options: \"\n",
    "    #             \"`underflow_overflow` (Detect underflow and overflow in activations and weights), \"\n",
    "    #             \"`tpu_metrics_debug` (print debug metrics on TPU).\"\n",
    "    #         )\n",
    "    #     })\n",
    "\n",
    "    # dataset\n",
    "    num_train_epochs: float = field(default=5)\n",
    "    per_device_train_batch_size: int = field(default=8)\n",
    "    dataloader_prefetch_factor: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\n",
    "           \"help\": \"Number of batches loaded in advance by each worker.\"\n",
    "        }\n",
    "    )\n",
    "    dataloader_num_workers: int = field(\n",
    "        default=2,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded\"\n",
    "                \" in the main process.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # learning rate\n",
    "    optim: Union[OptimizerNames, str] = field(default='adamw_torch', metadata={\"help\": \"The optimizer to use.\"}, )\n",
    "    learning_rate: float = field(default=3e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
    "    weight_decay: float = field(default=0.1, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n",
    "    lr_scheduler_type: Union[SchedulerType, str] = field(default=\"linear\")\n",
    "    warmup_ratio: float = field(\n",
    "        default=0.1,\n",
    "        metadata={\n",
    "           \"help\": \"Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # gradient\n",
    "    max_grad_norm: float = field(default=1.0, metadata={\"help\": \"Max gradient norm.\"})\n",
    "    gradient_accumulation_steps: int = field(default=1)  # if>1, real save/eval_steps = it * ori_steps\n",
    "\n",
    "    # eval and save\n",
    "    eval_strategy: Union[IntervalStrategy, str] = field(default='epoch')\n",
    "    save_strategy: Union[IntervalStrategy, str] = field(default=\"epoch\")\n",
    "    save_total_limit: Optional[int] = field(default=1)\n",
    "\n",
    "    # log/progress bar\n",
    "    log_level: Optional[str] = field(default=\"info\")\n",
    "    disable_tqdm: Optional[bool] = field(default=False)\n",
    "\n",
    "    # callback\n",
    "    restore_callback_states_from_checkpoint: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to restore the callback states from the checkpoint. If `True`, will override callbacks passed to the `Trainer` if they exist in the checkpoint.\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    max_seq_length: int = field(default=512)\n",
    "    padding: str = field(default='longest', metadata={\"help\": \"The padding strategy to use.(longest/max_length)\"})\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "        },\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=\"data/csts_train.csv\",\n",
    "        metadata={\"help\": \"A csv or a json file containing the training data.\"},\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=\"data/csts_validation.csv\",\n",
    "        metadata={\"help\": \"A csv or a json file containing the validation data.\"},\n",
    "    )\n",
    "    test_file: Optional[str] = field(\n",
    "        default=\"data/csts_test.csv\",\n",
    "        metadata={\"help\": \"A csv or a json file containing the test data.\"},\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class TokenizerAndModelArguments:\n",
    "    model_name_or_path: str = field(\n",
    "        default=\"princeton-nlp/sup-simcse-roberta-base\",\n",
    "        metadata={\n",
    "            \"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"\n",
    "        }\n",
    "    )\n",
    "    cl_temp: float = field(default=1.5, metadata={\"help\": \"Temperature for contrastive loss.\"})\n",
    "    freeze_encoder: Optional[bool] = field(\n",
    "        default=True, metadata={\"help\": \"Freeze encoder weights.\"}\n",
    "    )\n",
    "    transform: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Use a linear transformation on the encoder output\"},\n",
    "    )"
   ],
   "id": "bf30af0daa2d92b1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T14:03:07.222960Z",
     "start_time": "2025-01-11T14:03:06.307812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets.disable_progress_bars()\n",
    "raw_datasets = load_dataset(\"csv\", data_files={'train': 'data/csts_train.csv', 'test': 'data/csts_test.csv', 'validation': 'data/csts_validation.csv'})"
   ],
   "id": "4160523d5c37fb36",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T13:55:15.554026Z",
     "start_time": "2025-01-11T13:55:14.999274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def unbatch(examples):\n",
    "    out = {}\n",
    "    for k,v in examples.items():\n",
    "        tv = []\n",
    "        for iv in v:\n",
    "            tv = tv + iv\n",
    "        out[k] = tv\n",
    "    return out\n",
    "\n",
    "def scale_to_range(labels:List, scale:tuple):\n",
    "    min_, max_ = scale\n",
    "    return list(map(lambda x: (x - min_) / (max_ - min_), labels))\n",
    "\n",
    "def preprocess_func(examples, tokenizer: PreTrainedTokenizerBase,\n",
    "                    sentence1_key: str, sentence2_key: str, condition_key: str,\n",
    "                    similarity_key: str, scale: tuple):\n",
    "    sent1_args = (examples[sentence1_key], examples[condition_key])\n",
    "    sent2_args = (examples[sentence2_key], examples[condition_key])\n",
    "    sent1_res = tokenizer(*sent1_args, truncation=True)\n",
    "    sent2_res = tokenizer(*sent2_args, truncation=True)\n",
    "    for idx in [2, ]:\n",
    "        for key in sent2_res.keys():\n",
    "            sent1_res[key + '_' + str(idx)] = sent2_res[key]\n",
    "    sent1_res['labels'] = scale_to_range(examples[similarity_key], scale)\n",
    "    return sent1_res\n",
    "\n",
    "# dataset\n",
    "def str_if_contain_in_str_list(one_str:str, str_list:Iterable[str], mode:str='contain'):\n",
    "    \"\"\"\n",
    "    check if one_str in str_list\n",
    "    :param one_str:\n",
    "    :param str_list:\n",
    "    :param mode:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for one_str_in_list in str_list:\n",
    "        if mode=='contain':\n",
    "            if one_str_in_list in one_str:\n",
    "                return True\n",
    "        elif mode=='contained':\n",
    "            if one_str in one_str_in_list:\n",
    "                return True\n",
    "        else:\n",
    "            raise ValueError(f'mode {mode} not recognized')\n",
    "    return False\n",
    "\n",
    "def listdict_map_dictlist(listdict:Optional[List[Dict[str, Any]]]=None,\n",
    "                          dictlist:Optional[Dict[str, List[Any]]]=None):\n",
    "    \"\"\"\n",
    "    listdict: [{\"a\":1, \"b\":2}, {\"a\":3, \"b\":4}]\n",
    "    dictlist: {\"a\":[1,3], \"b\":[2,4]}\n",
    "    :param listdict:\n",
    "    :param dictlist:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    result = None\n",
    "    if listdict is not None:\n",
    "        result = {}\n",
    "        for one_dict in listdict:\n",
    "            for key, value in one_dict.items():\n",
    "                if key not in result.keys():\n",
    "                    result[key] = []\n",
    "                result[key].append(value)\n",
    "    elif dictlist is not None:\n",
    "        result = []\n",
    "        for key, value_list in dictlist.items():\n",
    "            for idx, value in enumerate(value_list):\n",
    "                if idx >= len(result):\n",
    "                    result.append({})\n",
    "                result[idx][key] = value\n",
    "    return result\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorWithPaddingForMultiRenameInputs:\n",
    "    group_feature_names: List[List[str]]\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "    model_input_names: tuple[str] = (\"input_ids\", \"token_type_ids\", \"attention_mask\")\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        all_input_keys = features[0].keys()\n",
    "        features: Dict[str, List[Any]] = listdict_map_dictlist(features,None)\n",
    "        other_keys_not_in_group_feature_names = []\n",
    "        for input_key in all_input_keys:\n",
    "            is_in_group = False\n",
    "            for one_group_keys in self.group_feature_names:\n",
    "                if input_key in one_group_keys:\n",
    "                    is_in_group = True\n",
    "                    break\n",
    "            if not is_in_group:\n",
    "                if str_if_contain_in_str_list(input_key, self.model_input_names, mode='contain'):\n",
    "                    pass # although not in group_feature_names, we really need to pad them\n",
    "                else:\n",
    "                    other_keys_not_in_group_feature_names.append(input_key)\n",
    "\n",
    "        batch = {}\n",
    "        for other_key in other_keys_not_in_group_feature_names:\n",
    "            batch[other_key] = features[other_key]\n",
    "        for gid, one_group_keys in enumerate(self.group_feature_names):\n",
    "            #  first, find group_key contain input_ids\n",
    "            one_group_input_ids = []\n",
    "            for group_key in one_group_keys:\n",
    "                if self.model_input_names[0] in group_key:\n",
    "                    one_group_input_ids.append(group_key)\n",
    "            count_input_ids = len(one_group_input_ids)\n",
    "\n",
    "            # second, expand complete group keys\n",
    "            complete_one_group_keys = one_group_input_ids.copy()\n",
    "            for input_ids_var in one_group_input_ids:\n",
    "                for stand_name in self.model_input_names[1:]:\n",
    "                    complete_one_group_keys.append(input_ids_var.replace(self.model_input_names[0], stand_name))\n",
    "\n",
    "            # third, intersection really input\n",
    "            complete_one_group_keys = set(complete_one_group_keys).intersection(all_input_keys)\n",
    "            complete_one_group_keys = sorted(list(complete_one_group_keys))\n",
    "\n",
    "            # forth, construct stand_name_list map original_name_list\n",
    "            original_name_list = complete_one_group_keys.copy()\n",
    "            stand_name_list = []\n",
    "            for original_name in original_name_list:\n",
    "                for stand_name in self.model_input_names:\n",
    "                    if stand_name in original_name:\n",
    "                        stand_name_list.append(stand_name)\n",
    "                        break\n",
    "            original_to_stand_map = dict(zip(original_name_list, stand_name_list))\n",
    "\n",
    "            # fifth, concat multiInput\n",
    "            original_name_features: Dict[str, List[Any]] = {}\n",
    "            for original_name in original_name_list:\n",
    "                original_name_features[original_name] = features[original_name]\n",
    "            group_features: Dict[str, List[Any]] = {key:[] for key in set(stand_name_list)}\n",
    "            for original_name in original_name_list:\n",
    "                group_features[original_to_stand_map[original_name]].extend(original_name_features[original_name])\n",
    "\n",
    "            # sixth, pad\n",
    "            group_batch = self.tokenizer.pad(group_features,\n",
    "                                             padding=self.padding,\n",
    "                                             max_length=self.max_length,\n",
    "                                             pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "                                             return_tensors=self.return_tensors)\n",
    "\n",
    "            # chunk\n",
    "            #if count_input_ids > 1:\n",
    "            per_chunk_size = group_batch[self.model_input_names[0]].shape[0] // count_input_ids\n",
    "            for stand_name in group_batch.keys():\n",
    "                corr_original_name_list = []\n",
    "                for original_name in original_name_list:\n",
    "                    if stand_name in original_name:\n",
    "                        corr_original_name_list.append(original_name)\n",
    "                corr_original_name_features_list = []\n",
    "                for idx in range(count_input_ids): #2  0/1\n",
    "                    corr_original_name_features_list.append(group_batch[stand_name][idx*per_chunk_size:(idx+1)*per_chunk_size])\n",
    "                batch.update(dict(zip(corr_original_name_list, corr_original_name_features_list)))\n",
    "        return BatchEncoding(batch, tensor_type=self.return_tensors)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('princeton-nlp/sup-simcse-roberta-base')\n",
    "label_unique = raw_datasets.unique('label')\n",
    "all_labels = set(label_unique['train']+label_unique['validation'])\n",
    "scale = (min(all_labels),max(all_labels))\n",
    "shuffle_datasets = raw_datasets.sort(['sentence1','label'], reverse=[False, True])['train'].batch(2).shuffle(42)\n",
    "trans_datasets = shuffle_datasets.map(unbatch, batched=True).map(preprocess_func, batched=True, fn_kwargs={\n",
    "    'tokenizer': tokenizer,\n",
    "    'sentence1_key': 'sentence1',\n",
    "    'sentence2_key': 'sentence2',\n",
    "    'condition_key': 'condition',\n",
    "    'similarity_key': 'label',\n",
    "    'scale': scale\n",
    "}, remove_columns=raw_datasets['train'].column_names)\n",
    "collate_fn = DataCollatorWithPaddingForMultiRenameInputs(\n",
    "    [['input_ids','input_ids_2']],\n",
    "    tokenizer, padding='longest', max_length=512, return_tensors='pt')\n",
    "dl = DataLoader(trans_datasets, batch_size=16, collate_fn=collate_fn)\n",
    "for ba in dl:\n",
    "    for key in ba.keys():\n",
    "        print(key, ba[key])\n",
    "    break\n",
    "    print('='*20)"
   ],
   "id": "89ea44307e77a218",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7500, 0.7500, 0.7500, 0.5000,\n",
      "        0.0000, 0.7500, 0.2500, 1.0000, 0.7500, 0.7500, 0.5000])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0]])\n",
      "attention_mask_2 tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0]])\n",
      "input_ids tensor([[    0,   250,   792, 10097,   443,    19,  8817,     9,    82,  3051,\n",
      "            11,   258,  9969,     8,   449,  5110,  4731, 15258,    11,     5,\n",
      "          4472,     4,     2,     2,   133, 21039,     4,     2,     1,     1],\n",
      "        [    0,   250,   792, 10097,   443,    19,  8817,     9,    82,  3051,\n",
      "            11,   258,  9969,     8,   449,  5110,  4731, 15258,    11,     5,\n",
      "          4472,     4,     2,     2,   133,   346,     9,    82,     4,     2],\n",
      "        [    0,   250,   313,    16,   602,    10,  2170,     9,    10, 34984,\n",
      "           661,  4655,    10,  7610,    11,     5,   935,   479,     2,     2,\n",
      "           133,  1907,     9, 21039,     4,     2,     1,     1,     1,     1],\n",
      "        [    0,   250,   313,    16,   602,    10,  2170,     9,    10, 34984,\n",
      "           661,  4655,    10,  7610,    11,     5,   935,   479,     2,     2,\n",
      "           133,   346,     9,    82,     4,     2,     1,     1,     1,     1],\n",
      "        [    0,  9058,   604,  2498,   543, 15146,     8,  3944, 20252,    32,\n",
      "          2934,    15,    10, 35737,   279,   479,     2,     2,   133,  4298,\n",
      "             9,     5,  1138,     4,     2,     1,     1,     1,     1,     1],\n",
      "        [    0,  9058,   604,  2498,   543, 15146,     8,  3944, 20252,    32,\n",
      "          2934,    15,    10, 35737,   279,   479,     2,     2,   133,  1907,\n",
      "             9,  1138,     4,     2,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   250,   664,   621,    11,    10,  3403,  8284,    34,    49,\n",
      "          3124,  3112,   479,     2,     2,   133,  1940,     4,     2,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   250,   664,   621,    11,    10,  3403,  8284,    34,    49,\n",
      "          3124,  3112,   479,     2,     2,   133,  3195,     9,  7626,     4,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   250,   313,   608,    10, 12800,  4929,  7610,    11,   935,\n",
      "           150,   145, 32779, 14677,   479,     2,     2,   133,   766,     9,\n",
      "             5,   177,     4,     2,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   250,   313,   608,    10, 12800,  4929,  7610,    11,   935,\n",
      "           150,   145, 32779, 14677,   479,     2,     2,   133,   766,     9,\n",
      "             5,   317,     4,     2,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 15622,  3678,   422,    15,  6964,  2156,    65, 32564,     7,\n",
      "          2916,  2440,  1011,   479,     2,     2,   133,  2574,     9,     5,\n",
      "           317,     4,     2,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 15622,  3678,   422,    15,  6964,  2156,    65, 32564,     7,\n",
      "          2916,  2440,  1011,   479,     2,     2,   133,  3477,    18,  2078,\n",
      "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   102, 18408,  1975, 14500,  4806,    16,  9755,   149,     5,\n",
      "           935,   227,    80,  2301,     9,  1275,  7898,   479,     2,     2,\n",
      "           133,  2414,     4,     2,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   102, 18408,  1975, 14500,  4806,    16,  9755,   149,     5,\n",
      "           935,   227,    80,  2301,     9,  1275,  7898,   479,     2,     2,\n",
      "           133,   317,     4,     2,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   250, 31053,  1104,  2335,    16,  3406,    10,  4757,    11,\n",
      "            63,  6085,   149,     5,  6964,   479,     2,     2, 48674,     9,\n",
      "          3477,     4,     2,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   250, 31053,  1104,  2335,    16,  3406,    10,  4757,    11,\n",
      "            63,  6085,   149,     5,  6964,   479,     2,     2,   133,  3195,\n",
      "             9,  3477,     4,     2,     1,     1,     1,     1,     1,     1]])\n",
      "input_ids_2 tensor([[    0,   133,  6360,    23,     5,  4105,    16,  1658,    19, 22239,\n",
      "           449,  5110,   217, 28522,     8,  3539, 14216,  1980,     4,     2,\n",
      "             2,   133, 21039,     4,     2,     1,     1,     1,     1,     1],\n",
      "        [    0,   133,  6360,    23,     5,  4105,    16,  1658,    19, 22239,\n",
      "           449,  5110,   217, 28522,     8,  3539, 14216,  1980,     4,     2,\n",
      "             2,   133,   346,     9,    82,     4,     2,     1,     1,     1],\n",
      "        [    0,   250,   313,  7001,    11,    10,  5851,   438, 20093,  6399,\n",
      "             8,   909,  9304, 14023,    41, 14278, 16391,    15,    10, 14678,\n",
      "           479,     2,     2,   133,  1907,     9, 21039,     4,     2,     1],\n",
      "        [    0,   250,   313,  7001,    11,    10,  5851,   438, 20093,  6399,\n",
      "             8,   909,  9304, 14023,    41, 14278, 16391,    15,    10, 14678,\n",
      "           479,     2,     2,   133,   346,     9,    82,     4,     2,     1],\n",
      "        [    0,  6323,  1663,  1138,    32,    15,    10,  1761,   608,   173,\n",
      "             7,    10,   745,   479,     2,     2,   133,  4298,     9,     5,\n",
      "          1138,     4,     2,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,  6323,  1663,  1138,    32,    15,    10,  1761,   608,   173,\n",
      "             7,    10,   745,   479,     2,     2,   133,  1907,     9,  1138,\n",
      "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   102,  2143,    11,    41,  8978, 11458,    16,    59,     7,\n",
      "          3242,    10,  3403,   479,     2,     2,   133,  1940,     4,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   102,  2143,    11,    41,  8978, 11458,    16,    59,     7,\n",
      "          3242,    10,  3403,   479,     2,     2,   133,  3195,     9,  7626,\n",
      "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 29186,   877,  4929,   254, 13855,    10,  4044,   150,   145,\n",
      "         16372,     2,     2,   133,   766,     9,     5,   177,     4,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 29186,   877,  4929,   254, 13855,    10,  4044,   150,   145,\n",
      "         16372,     2,     2,   133,   766,     9,     5,   317,     4,     2,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0, 15622,  3678,   310,    19,   349,    97,    66,    11,     5,\n",
      "           882,   479,     2,     2,   133,  2574,     9,     5,   317,     4,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   495, 17821,  7031,  8173,    15,  2084, 14829,  1567,   349,\n",
      "            97,  2156,    25,  2203,  1656,  4102,    15,  6964,   479,     2,\n",
      "             2,   133,  3477,    18,  2078,     4,     2,     1,     1,     1],\n",
      "        [    0,   250,  4243, 19297,   661,    16,  4731,   149,     5,   935,\n",
      "          1065,    10, 10667,  2718,   479,     2,     2,   133,  2414,     4,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   250,  4243, 19297,   661,    16,  4731,   149,     5,   935,\n",
      "          1065,    10, 10667,  2718,   479,     2,     2,   133,   317,     4,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   250,  2335,  9755,    62,    11,     5,   935,   220,     7,\n",
      "            10,  1104,   790,   479,     2,     2, 48674,     9,  3477,     4,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1],\n",
      "        [    0,   250,  1104, 20830,  5792,   159,    10, 10667,  2718, 12564,\n",
      "            39, 32652,    15,     5,  1255,   639,   123,   479,     2,     2,\n",
      "           133,  3195,     9,  3477,     4,     2,     1,     1,     1,     1]])\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-11T05:44:13.700782Z",
     "start_time": "2025-01-11T05:44:08.463014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def scale_to_range(labels:List, scale:tuple):\n",
    "    min_, max_ = scale\n",
    "    return list(map(lambda x: (x - min_) / (max_ - min_), labels))\n",
    "    \n",
    "def get_preprocess_func(tokenizer:PreTrainedTokenizerBase, sentence1_key:str, sentence2_key:str, condition_key:str, similarity_key:str, scale:tuple):\n",
    "    \n",
    "    def preprocess_func(examples):\n",
    "        sent1_args = (examples[sentence1_key], examples[condition_key])\n",
    "        sent2_args = (examples[sentence2_key], examples[condition_key])\n",
    "        sent1_res = tokenizer(*sent1_args, truncation=True)\n",
    "        sent2_res = tokenizer(*sent2_args, truncation=True)\n",
    "        for idx in [2,]:\n",
    "            for key in sent2_res.keys():\n",
    "                sent1_res[key+'_'+str(idx)] = sent2_res[key]\n",
    "        sent1_res['labels'] = scale_to_range(examples[similarity_key], scale)\n",
    "        return sent1_res\n",
    "    return preprocess_func\n",
    "\n",
    "def add_prefix(examples, prefix:str, columns:Optional[Union[str, List[str]]]=None):\n",
    "    if isinstance(columns, str):\n",
    "        examples[prefix+'_'+columns] = examples[columns]\n",
    "        return examples\n",
    "    elif columns is None or columns == 'all':\n",
    "        columns = list(examples.keys())\n",
    "    for key in columns:\n",
    "        examples[prefix+'_'+key] = examples[key]\n",
    "    return examples\n",
    "\n",
    "def concat_pos_and_neg_datasets(datasets:Dataset):\n",
    "    pos = datasets.shard(2,0).map(add_prefix, batched=True, remove_columns=datasets.column_names, fn_kwargs={'prefix':'pos', 'columns':None})\n",
    "    neg = datasets.shard(2,1).map(add_prefix, batched=True, remove_columns=datasets.column_names, fn_kwargs={'prefix':'neg', 'columns':None})\n",
    "    new_datasets = concatenate_datasets([pos, neg], axis=1)\n",
    "    return new_datasets\n",
    "\n",
    "seed = 42\n",
    "transformers.enable_full_determinism(seed)\n",
    "datasets.disable_caching()\n",
    "datasets.disable_progress_bars()\n",
    "raw_datasets = load_dataset(\"csv\", data_files={'train': 'data/csts_train.csv', 'test': 'data/csts_test.csv', 'validation': 'data/csts_validation.csv'})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('princeton-nlp/sup-simcse-roberta-base')\n",
    "label_unique = raw_datasets.unique('label')\n",
    "all_labels = set(label_unique['train']+label_unique['validation'])\n",
    "scale = (min(all_labels),max(all_labels))\n",
    "trans_datasets = raw_datasets.sort(['sentence1','label'], reverse=[False, True]).map(get_preprocess_func(tokenizer, 'sentence1', 'sentence2','condition','label', scale), batched=True, remove_columns=raw_datasets['train'].column_names)\n",
    "\n",
    "new_trans_datasets = {}\n",
    "for key in trans_datasets.keys():\n",
    "    new_trans_datasets[key] = concat_pos_and_neg_datasets(trans_datasets[key])\n",
    "new_trans_datasets = DatasetDict(new_trans_datasets)\n",
    "new_trans_datasets = new_trans_datasets.shuffle(seed)\n",
    "new_trans_datasets['train']"
   ],
   "id": "6931c27606be52a0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['pos_input_ids', 'pos_attention_mask', 'pos_input_ids_2', 'pos_attention_mask_2', 'pos_labels', 'neg_input_ids', 'neg_attention_mask', 'neg_input_ids_2', 'neg_attention_mask_2', 'neg_labels'],\n",
       "    num_rows: 5671\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
