_attn_implementation_autoset: false
_name_or_path: princeton-nlp/sup-simcse-roberta-base
accelerator_config:
  dispatch_batches: null
  even_batches: true
  gradient_accumulation_kwargs: null
  non_blocking: false
  split_batches: false
  use_seedable_sampler: true
adafactor: false
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-08
add_cross_attention: false
architectures:
- RobertaModel
attention_probs_dropout_prob: 0.1
auto_find_batch_size: false
average_tokens_across_devices: false
bad_words_ids: null
batch_eval_metrics: false
begin_suppress_tokens: null
bf16: false
bf16_full_eval: false
bos_token_id: 0
chunk_size_feed_forward: 0
cl_temp: 1.5
classifier_dropout: null
cross_attention_hidden_size: null
data_seed: null
dataloader_drop_last: false
dataloader_num_workers: 2
dataloader_persistent_workers: false
dataloader_pin_memory: true
dataloader_prefetch_factor: 1
ddp_backend: null
ddp_broadcast_buffers: null
ddp_bucket_cap_mb: null
ddp_find_unused_parameters: null
ddp_timeout: 1800
debug: []
decoder_start_token_id: null
deepspeed: null
disable_tqdm: false
dispatch_batches: null
diversity_penalty: 0.0
do_eval: true
do_predict: true
do_sample: false
do_train: true
early_stopping: false
early_stopping_patience: 10
encoder_no_repeat_ngram_size: 0
eos_token_id: 2
eval_accumulation_steps: null
eval_delay: 0
eval_do_concat_batches: true
eval_loss:
- 0.8236508965492249
eval_on_start: false
eval_pearsonr:
- -0.06024017758542549
eval_runtime:
- 25.9536
eval_samples_per_second:
- 0.616
eval_spearmanr:
- 0.01263017955901345
eval_steps: 500
eval_steps_per_second:
- 0.077
eval_strategy: steps
eval_use_gather_object: false
evaluation_strategy: null
exponential_decay_length_penalty: null
finetuning_task: null
forced_bos_token_id: null
forced_eos_token_id: null
fp16: false
fp16_backend: auto
fp16_full_eval: false
fp16_opt_level: O1
freeze_encoder: true
fsdp: []
fsdp_config:
  min_num_params: 0
  xla: false
  xla_fsdp_grad_ckpt: false
  xla_fsdp_v2: false
fsdp_min_num_params: 0
fsdp_transformer_layer_cls_to_wrap: null
full_determinism: false
gradient_accumulation_steps: 1
gradient_checkpointing: false
gradient_checkpointing_kwargs: null
greater_is_better: true
group_by_length: false
half_precision_backend: auto
hidden_act: gelu
hidden_dropout_prob: 0.1
hidden_size: 768
hub_always_push: false
hub_model_id: null
hub_private_repo: null
hub_strategy: every_save
hub_token: <HUB_TOKEN>
id2label:
  0: LABEL_0
  1: LABEL_1
ignore_data_skip: true
include_for_metrics: []
include_inputs_for_metrics: false
include_num_input_tokens_seen: false
include_tokens_per_second: false
initializer_range: 0.02
intermediate_size: 3072
is_decoder: false
is_encoder_decoder: false
jit_mode_eval: false
label2id:
  LABEL_0: 0
  LABEL_1: 1
label_names: null
label_smoothing_factor: 0.0
layer_norm_eps: 1.0e-05
learning_rate: 3.0e-05
length_column_name: length
length_penalty: 1.0
load_best_model_at_end: true
local_rank: 0
log_level: info
log_level_replica: warning
log_on_each_node: true
logging_dir: output/0\runs\Jan22_17-34-53_DESKTOP-SL7GB6I
logging_first_step: false
logging_nan_inf_filter: true
logging_steps: 500
logging_strategy: steps
lr_scheduler_kwargs: {}
lr_scheduler_type: linear
max_eval_samples: 16
max_grad_norm: 1.0
max_length: 20
max_position_embeddings: 514
max_predict_samples: null
max_seq_length: 512
max_steps: -1
max_train_samples: 16
metric_for_best_model: eval_spearmanr
min_length: 0
model_name_or_path: princeton-nlp/sup-simcse-roberta-base
model_type: roberta
mp_parameters: ''
n_trials: 5
neftune_noise_alpha: null
no_cuda: false
no_repeat_ngram_size: 0
num_attention_heads: 12
num_beam_groups: 1
num_beams: 1
num_hidden_layers: 12
num_return_sequences: 1
num_train_epochs: 3
optim: adamw_torch
optim_args: null
optim_target_modules: null
output_attentions: false
output_dir: output/0
output_hidden_states: false
output_scores: false
overwrite_output_dir: true
pad_token_id: 1
padding: longest
past_index: -1
per_device_eval_batch_size: 8
per_device_train_batch_size: 8
per_gpu_eval_batch_size: null
per_gpu_train_batch_size: null
pool_type: cls
position_embedding_type: absolute
prediction_loss_only: false
prefix: null
problem_type: null
pruned_heads: {}
push_to_hub: false
push_to_hub_model_id: null
push_to_hub_organization: null
push_to_hub_token: <PUSH_TO_HUB_TOKEN>
ray_scope: last
remove_invalid_values: false
remove_unused_columns: true
repetition_penalty: 1.0
report_to:
- tensorboard
restore_callback_states_from_checkpoint: true
resume_from_checkpoint: null
return_dict: true
return_dict_in_generate: false
run_name: output/0
run_results_dir: ./run_results/
run_times: 1
save_on_each_node: false
save_only_model: false
save_safetensors: true
save_steps: 500
save_strategy: steps
save_total_limit: 1
seed: 42
sep_token_id: null
skip_memory_metrics: true
split_batches: null
suppress_tokens: null
task_specific_params: null
temperature: 1.0
test_file: data/csts_test.csv
tf32: null
tf_legacy_loss: false
tie_encoder_decoder: false
tie_word_embeddings: true
tokenizer_class: null
top_k: 50
top_p: 1.0
torch_compile: false
torch_compile_backend: null
torch_compile_mode: null
torch_dtype: null
torch_empty_cache_steps: null
torchdynamo: null
torchscript: false
tpu_metrics_debug: false
tpu_num_cores: null
train_file: data/csts_train.csv
train_with_lora: true
transform: true
transformers_version: 4.48.1
tuner: false
tuner_cl_temp:
  high: 1.5
  low: 0.5
  step: 0.1
  type: float
tuner_pool_type:
  choices:
  - cls
  - avg
  - avg_top2
  - avg_first_last
  type: categorical
tuner_results_dir: ./tuner_results/
type_vocab_size: 1
typical_p: 1.0
use_bfloat16: false
use_cache: true
use_cpu: false
use_ipex: false
use_legacy_prediction_loop: false
use_liger_kernel: false
use_mps_device: false
validation_file: data/csts_validation.csv
vocab_size: 50265
warmup_ratio: 0.1
warmup_steps: 0
weight_decay: 0.1
